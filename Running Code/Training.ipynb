{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Experiments with Article  Content ONLY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-22T20:21:33.128159Z",
     "start_time": "2019-03-22T20:21:33.120163Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tooba\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1212: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "from util import *\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We are loading the data from both dawn and express tribune."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-22T20:21:33.845757Z",
     "start_time": "2019-03-22T20:21:33.781776Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('articles-annotated.csv',encoding = \"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop rows where no label given\n",
    "data = data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-22T20:21:34.563607Z",
     "start_time": "2019-03-22T20:21:34.147730Z"
    }
   },
   "outputs": [],
   "source": [
    "# Converting label from floats to int\n",
    "data['Label'] = data['Label'].astype(int)\n",
    "# Saving cleaned data\n",
    "data.to_csv('cleaned-data.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-22T20:21:34.607596Z",
     "start_time": "2019-03-22T20:21:34.567606Z"
    }
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('cleaned-data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-22T20:21:35.332994Z",
     "start_time": "2019-03-22T20:21:35.313003Z"
    }
   },
   "outputs": [],
   "source": [
    "def remove_non_ascii(words):\n",
    "    words = [unicodedata.normalize('NFKD', word).encode('ascii', 'ignore').decode('utf-8', 'ignore') for word in words]\n",
    "    return words\n",
    "# The first pre-processing step which we will do is transform our tweets into lower case.\n",
    "# This avoids having multiple copies of the same words\n",
    "def to_lowercase(words):\n",
    "    words = [word.lower() for word in words]\n",
    "    return words\n",
    "# Removing punctuation to reduce the amount of the training data\n",
    "def remove_punctuation(words):\n",
    "    words = [re.sub(r'[^\\w\\s]', '', word) for word in words if re.sub(r'[^\\w\\s]', '', word) not in '']    \n",
    "    return words\n",
    "# Removing numbers from data since they aren't useful in this context.\n",
    "def replace_numbers(words):\n",
    "    words = [re.sub(r'[^\\w\\s]', '', word) for word in words]\n",
    "    return words\n",
    "def remove_stopwords(words):\n",
    "    words = [word for word in words if word not in stopwords.words('english')]\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-22T20:21:35.810332Z",
     "start_time": "2019-03-22T20:21:35.798335Z"
    }
   },
   "outputs": [],
   "source": [
    "def normalize(words):\n",
    "    words = remove_non_ascii(words)\n",
    "    words = to_lowercase(words)\n",
    "    words = remove_punctuation(words)\n",
    "    words = replace_numbers(words)\n",
    "    words = remove_stopwords(words)\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-22T20:23:09.023671Z",
     "start_time": "2019-03-22T20:21:36.539016Z"
    }
   },
   "outputs": [],
   "source": [
    "data['tokenized_Content'] = data['Content'].apply(word_tokenize)\n",
    "data['tokenized_Content'] = data['tokenized_Content'].apply(normalize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-22T20:23:09.187625Z",
     "start_time": "2019-03-22T20:23:09.031668Z"
    }
   },
   "outputs": [],
   "source": [
    "# Saving the processed data to a csv file\n",
    "data.to_csv(\"preprocessed.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-22T20:23:09.251618Z",
     "start_time": "2019-03-22T20:23:09.191620Z"
    }
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('preprocessed.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression with TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-22T20:52:51.996262Z",
     "start_time": "2019-03-22T20:52:51.960272Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train (179,)\n",
      "X_test (45,)\n",
      "y_train (179,)\n",
      "y_test (45,)\n"
     ]
    }
   ],
   "source": [
    "X = data['tokenized_Content']\n",
    "y = data['Label']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.2, random_state=1234)\n",
    "print('X_train',X_train.shape)\n",
    "print('X_test',X_test.shape)\n",
    "print('y_train',y_train.shape)\n",
    "print('y_test',y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-22T21:40:57.060422Z",
     "start_time": "2019-03-22T21:40:56.719625Z"
    }
   },
   "outputs": [],
   "source": [
    "#The following parameters were the best according to our dataset\n",
    "tdf = TfidfVectorizer(stop_words='english',max_df = 0.65,min_df = 0.001,ngram_range=(1,1))\n",
    "vectorizer = tdf.fit(X_train)\n",
    "train_transformed = vectorizer.transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-22T21:41:07.858848Z",
     "start_time": "2019-03-22T21:41:07.817874Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tooba\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning:\n",
      "\n",
      "Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "\n",
      "C:\\Users\\tooba\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:459: FutureWarning:\n",
      "\n",
      "Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression(penalty = 'l1')\n",
    "model = model.fit(train_transformed , y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving tfidf model\n",
    "pickle.dump(vectorizer, open(\"tfidf.pickle\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-22T21:51:23.442923Z",
     "start_time": "2019-03-22T21:51:23.317001Z"
    }
   },
   "outputs": [],
   "source": [
    "#saving logistic regression\n",
    "filename = 'logistic_regression_TFIDF.sav'\n",
    "pickle.dump(model, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Statistics\n",
    "- Uncomment to see the model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-22T18:43:19.242243Z",
     "start_time": "2019-03-22T18:43:19.238244Z"
    }
   },
   "outputs": [],
   "source": [
    "# # Score only on the test set. NO CV\n",
    "# print (\"Logistig Regression: \\n\")\n",
    "# print ( \"F1 score {:.4}%\".format(f1_score(y_test, predicted, average='macro')*100 ) )\n",
    "# print ( \"Accuracy score {:.4}%\\n\\n\".format(accuracy_score(y_test, predicted)*100) )\n",
    "# error = mean_squared_error(y_test, predicted)\n",
    "# print('MSE',error)\n",
    "# print(metrics.classification_report(y_test, predicted,target_names=['Fake','Unverified','Real']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
